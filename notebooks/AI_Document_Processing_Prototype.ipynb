{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac5766cc",
   "metadata": {},
   "source": [
    "\n",
    "# AI-Powered Document Processing — Prototype Notebook\n",
    "\n",
    "This notebook is a **working prototype** of a production-grade document processing pipeline as described in the myOnsite Healthcare case study.  \n",
    "It demonstrates an **end-to-end flow**: ingestion → OCR/text extraction → classification → entity extraction → validation & confidence → enrichment → routing → metrics/export.\n",
    "\n",
    "> ⚠️ Notes  \n",
    "> - Heavy/enterprise components are **stubbed** with clean interfaces so you can **hot-swap** engines later (Textract/Vision/Claude/GPT-4V, Kafka/Ray, etc.).  \n",
    "> - All code runs locally with minimal deps. Optional features fall back gracefully if a dependency/model is missing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b25bda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) Imports & Setup\n",
    "\n",
    "import os, re, json, time, uuid, glob, random, math, datetime as dt\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Optional imports (safe fallbacks)\n",
    "try:\n",
    "    import pdfplumber\n",
    "except Exception as e:\n",
    "    pdfplumber = None\n",
    "\n",
    "try:\n",
    "    from PIL import Image\n",
    "except Exception as e:\n",
    "    Image = None\n",
    "\n",
    "try:\n",
    "    import pytesseract\n",
    "except Exception as e:\n",
    "    pytesseract = None\n",
    "\n",
    "# spaCy is optional. If not installed, we will use regex NER\n",
    "try:\n",
    "    import spacy\n",
    "    _NLP = spacy.load(\"en_core_web_sm\")\n",
    "except Exception as e:\n",
    "    spacy = None\n",
    "    _NLP = None\n",
    "\n",
    "print(\"pdfplumber:\", bool(pdfplumber), \"| PIL:\", bool(Image), \"| pytesseract:\", bool(pytesseract), \"| spaCy model:\", bool(_NLP))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f67ec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2) Config & Pluggable Engines\n",
    "\n",
    "class Config:\n",
    "    DATA_DIR = Path(\"../data\")          # put sample PDFs/images here\n",
    "    OUTPUT_DIR = Path(\"../outputs\")\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Thresholds\n",
    "    MIN_CONFIDENCE_ROUTE = 0.88   # below → route to human\n",
    "    MIN_FIELD_CONF = 0.7\n",
    "    \n",
    "    # Active engines (can be switched)\n",
    "    OCR_ENGINE = \"pdfplumber|tesseract\" # try pdfplumber text, else tesseract for images\n",
    "    CLASSIFIER = \"rule_based\"           # or \"sklearn\" (placeholder)\n",
    "    NER_ENGINE = \"spacy|regex\"          # try spaCy, else regex fallback\n",
    "\n",
    "CONFIG = Config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf21b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3) Utilities\n",
    "\n",
    "def read_binary(path: Path) -> bytes:\n",
    "    with open(path, \"rb\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def guess_mime(path: Path) -> str:\n",
    "    ext = path.suffix.lower()\n",
    "    if ext in [\".pdf\"]: return \"application/pdf\"\n",
    "    if ext in [\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\"]: return \"image\"\n",
    "    if ext in [\".txt\"]: return \"text\"\n",
    "    if ext in [\".doc\", \".docx\"]: return \"word\"\n",
    "    return \"unknown\"\n",
    "\n",
    "def now_iso():\n",
    "    return dt.datetime.utcnow().isoformat() + \"Z\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24358fbd",
   "metadata": {},
   "source": [
    "### 4) OCR/Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0343d965",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_text_pdf(path: Path) -> str:\n",
    "    if not pdfplumber:\n",
    "        return \"\"\n",
    "    text_chunks = []\n",
    "    with pdfplumber.open(str(path)) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            txt = page.extract_text(x_tolerance=1.5, y_tolerance=1.5) or \"\"\n",
    "            text_chunks.append(txt)\n",
    "    return \"\\n\".join(text_chunks)\n",
    "\n",
    "def extract_tables_pdf(path: Path) -> List[pd.DataFrame]:\n",
    "    if not pdfplumber:\n",
    "        return []\n",
    "    tables = []\n",
    "    with pdfplumber.open(str(path)) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            for table in page.extract_tables() or []:\n",
    "                try:\n",
    "                    df = pd.DataFrame(table[1:], columns=table[0])\n",
    "                except Exception:\n",
    "                    df = pd.DataFrame(table)\n",
    "                tables.append(df)\n",
    "    return tables\n",
    "\n",
    "def extract_text_image(path: Path) -> str:\n",
    "    if not (Image and pytesseract):\n",
    "        return \"\"\n",
    "    img = Image.open(path)\n",
    "    return pytesseract.image_to_string(img)\n",
    "\n",
    "def extract_text(path: Path) -> Tuple[str, List[pd.DataFrame]]:\n",
    "    mime = guess_mime(path)\n",
    "    text, tables = \"\", []\n",
    "    if mime == \"application/pdf\":\n",
    "        text = extract_text_pdf(path)\n",
    "        tables = extract_tables_pdf(path)\n",
    "    elif mime == \"image\":\n",
    "        text = extract_text_image(path)\n",
    "    elif mime == \"text\":\n",
    "        text = path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    else:\n",
    "        text = \"\"  # unsupported in this prototype\n",
    "    return text.strip(), tables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e541e884",
   "metadata": {},
   "source": [
    "### 5) Classification (Rule-based fallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07e3061",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "INVOICE_HINTS = [\"invoice\", \"amount due\", \"bill to\", \"subtotal\", \"total\", \"tax\"]\n",
    "FORM_HINTS = [\"form\", \"name:\", \"email\", \"phone\", \"address\"]\n",
    "CONTRACT_HINTS = [\"agreement\", \"party\", \"contract\", \"effective date\", \"term\"]\n",
    "\n",
    "def classify_document(text: str) -> str:\n",
    "    t = text.lower()\n",
    "    score = {\"invoice\":0, \"form\":0, \"contract\":0, \"unknown\":0}\n",
    "    score[\"invoice\"] += sum(h in t for h in INVOICE_HINTS)\n",
    "    score[\"form\"] += sum(h in t for h in FORM_HINTS)\n",
    "    score[\"contract\"] += sum(h in t for h in CONTRACT_HINTS)\n",
    "    # pick top class\n",
    "    cls = max(score, key=score.get)\n",
    "    return cls if score[cls] > 0 else \"unknown\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8482f8e2",
   "metadata": {},
   "source": [
    "### 6) Entity Extraction (spaCy if available; regex fallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f9fe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MONEY_RE = re.compile(r\"(?:USD\\s?)?\\$\\s?([0-9]{1,3}(?:,[0-9]{3})*(?:\\.[0-9]{2})?)\")\n",
    "DATE_RE = re.compile(r\"\\b(?:\\d{1,2}[/.-]){2}\\d{2,4}|\\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\.?\\s+\\d{1,2},\\s*\\d{4}\")\n",
    "EMAIL_RE = re.compile(r\"[\\w\\.-]+@[\\w\\.-]+\")\n",
    "PHONE_RE = re.compile(r\"(?:\\+?\\d{1,3}[\\s-]?)?(?:\\(\\d{3}\\)|\\d{3})[\\s-]?\\d{3}[\\s-]?\\d{4}\")\n",
    "\n",
    "def extract_entities_regex(text: str) -> Dict[str, List[str]]:\n",
    "    ents = {\n",
    "        \"MONEY\": MONEY_RE.findall(text),\n",
    "        \"DATE\": DATE_RE.findall(text),\n",
    "        \"EMAIL\": EMAIL_RE.findall(text),\n",
    "        \"PHONE\": PHONE_RE.findall(text),\n",
    "    }\n",
    "    return ents\n",
    "\n",
    "def extract_entities_spacy(text: str) -> Dict[str, List[str]]:\n",
    "    if not _NLP:\n",
    "        return extract_entities_regex(text)\n",
    "    doc = _NLP(text)\n",
    "    ents = {}\n",
    "    for e in doc.ents:\n",
    "        ents.setdefault(e.label_, []).append(e.text)\n",
    "    # also add regex helpers\n",
    "    for k,v in extract_entities_regex(text).items():\n",
    "        ents.setdefault(k, [])\n",
    "        ents[k].extend(v)\n",
    "    return ents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572ed187",
   "metadata": {},
   "source": [
    "### 7) Schema Mapping, Validation & Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bacf276",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "INVOICE_SCHEMA = {\n",
    "    \"company_name\": {\"required\": True},\n",
    "    \"invoice_number\": {\"required\": True},\n",
    "    \"invoice_date\": {\"required\": True},\n",
    "    \"total_amount\": {\"required\": True},\n",
    "}\n",
    "\n",
    "def map_invoice_fields(text: str, ents: Dict[str, List[str]]) -> Dict[str, Any]:\n",
    "    # naive demo mappers\n",
    "    fields = {}\n",
    "    # company name heuristic\n",
    "    m = re.search(r\"(?i)(?:company|vendor|bill from)[:\\s]+([A-Za-z0-9&.,'\\-\\s]{3,})\", text)\n",
    "    if m:\n",
    "        fields[\"company_name\"] = m.group(1).strip()\n",
    "    # invoice number heuristic\n",
    "    m = re.search(r\"(?i)(invoice\\s*(?:no|number|#)[:\\s]*)([A-Za-z0-9-]+)\", text)\n",
    "    if m:\n",
    "        fields[\"invoice_number\"] = m.group(2).strip()\n",
    "    # date & total amount\n",
    "    fields[\"invoice_date\"] = ents.get(\"DATE\", [None])[0]\n",
    "    money_list = ents.get(\"MONEY\", [])\n",
    "    fields[\"total_amount\"] = max(money_list, default=None) if money_list else None\n",
    "    return fields\n",
    "\n",
    "def validate_fields(fields: Dict[str, Any], schema: Dict[str, Dict[str, Any]]) -> Tuple[bool, Dict[str, float]]:\n",
    "    conf = {}\n",
    "    valid = True\n",
    "    for key, rule in schema.items():\n",
    "        val = fields.get(key)\n",
    "        req = rule.get(\"required\", False)\n",
    "        present = val not in (None, \"\", [], {})\n",
    "        if req and not present:\n",
    "            valid = False\n",
    "            conf[key] = 0.0\n",
    "        else:\n",
    "            # trivial confidence: length/format heuristics\n",
    "            if key.endswith(\"date\") and val:\n",
    "                conf[key] = 0.9\n",
    "            elif key.endswith(\"amount\") and val:\n",
    "                conf[key] = 0.92\n",
    "            elif val:\n",
    "                conf[key] = 0.85\n",
    "            else:\n",
    "                conf[key] = 0.0\n",
    "    return valid, conf\n",
    "\n",
    "def aggregate_confidence(conf_map: Dict[str, float]) -> float:\n",
    "    if not conf_map: \n",
    "        return 0.0\n",
    "    vals = list(conf_map.values())\n",
    "    return float(np.mean(vals))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab1c3ce",
   "metadata": {},
   "source": [
    "### 8) Enrichment (Knowledge Graph stub) & Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77570e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "VENDOR_DB = {\n",
    "    \"myOnsite Healthcare LLC\": {\"vendor_id\": \"VEND-001\", \"domain\": \"healthcare\"},\n",
    "    \"Healthcare Solutions Inc.\": {\"vendor_id\": \"VEND-002\", \"domain\": \"healthcare\"},\n",
    "}\n",
    "\n",
    "def enrich(fields: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    company = fields.get(\"company_name\")\n",
    "    if company and company in VENDOR_DB:\n",
    "        fields.update(VENDOR_DB[company])\n",
    "    return fields\n",
    "\n",
    "def route_decision(doc_conf: float, business_critical: bool = False) -> str:\n",
    "    if business_critical and doc_conf < 0.95:\n",
    "        return \"human_review\"\n",
    "    if doc_conf < Config.MIN_CONFIDENCE_ROUTE:\n",
    "        return \"human_review\"\n",
    "    return \"auto_approve\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a937352",
   "metadata": {},
   "source": [
    "### 9) End-to-End Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4c3de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_document(path: Path) -> Dict[str, Any]:\n",
    "    doc_id = str(uuid.uuid4())\n",
    "    text, tables = extract_text(path)\n",
    "    cls = classify_document(text) if text else \"unknown\"\n",
    "    ents = extract_entities_spacy(text) if text else {}\n",
    "    \n",
    "    mapped = {}\n",
    "    confmap = {}\n",
    "    if cls == \"invoice\":\n",
    "        mapped = map_invoice_fields(text, ents)\n",
    "        valid, confmap = validate_fields(mapped, INVOICE_SCHEMA)\n",
    "    else:\n",
    "        # generic fallback\n",
    "        mapped = {\"raw_excerpt\": text[:500] if text else \"\"}\n",
    "        valid, confmap = True, {\"raw_excerpt\": 0.6 if text else 0.0}\n",
    "    \n",
    "    mapped = enrich(mapped)\n",
    "    doc_conf = aggregate_confidence(confmap)\n",
    "    route = route_decision(doc_conf, business_critical=False)\n",
    "    \n",
    "    result = {\n",
    "        \"doc_id\": doc_id,\n",
    "        \"path\": str(path),\n",
    "        \"mime\": guess_mime(path),\n",
    "        \"class\": cls,\n",
    "        \"entities\": ents,\n",
    "        \"fields\": mapped,\n",
    "        \"field_confidence\": confmap,\n",
    "        \"document_confidence\": round(doc_conf, 3),\n",
    "        \"route\": route,\n",
    "        \"processed_at\": now_iso(),\n",
    "    }\n",
    "    return result\n",
    "\n",
    "def batch_process(data_dir: Path = CONFIG.DATA_DIR) -> pd.DataFrame:\n",
    "    files = []\n",
    "    for ext in (\"*.pdf\",\"*.png\",\"*.jpg\",\"*.jpeg\",\"*.tif\",\"*.tiff\",\"*.txt\"):\n",
    "        files.extend(Path(data_dir).glob(ext))\n",
    "    files = sorted(files)\n",
    "    results = []\n",
    "    for p in files:\n",
    "        try:\n",
    "            res = process_document(p)\n",
    "            results.append(res)\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                \"doc_id\": str(uuid.uuid4()),\n",
    "                \"path\": str(p),\n",
    "                \"error\": str(e),\n",
    "                \"processed_at\": now_iso(),\n",
    "            })\n",
    "    df = pd.DataFrame(results)\n",
    "    out_csv = CONFIG.OUTPUT_DIR / f\"results_{int(time.time())}.csv\"\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"Saved results -> {out_csv}\")\n",
    "    return df\n",
    "\n",
    "# Quick smoke test on empty data dir (user should add sample files later)\n",
    "df = batch_process(CONFIG.DATA_DIR)\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c8c3fd",
   "metadata": {},
   "source": [
    "### 10) (Optional) FastAPI stub to connect the HTML dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b22d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This cell shows how you'd expose the pipeline as an API (run separately).\n",
    "# Save as: src/api.py and run: uvicorn src.api:app --reload\n",
    "\n",
    "FASTAPI_SNIPPET = r\"\"\"\n",
    "from fastapi import FastAPI, UploadFile, File\n",
    "from pathlib import Path\n",
    "import uuid, os\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "\n",
    "from .pipeline import process_document  # you can move the functions into src/pipeline.py\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "UPLOAD_DIR = Path(\"./uploads\")\n",
    "UPLOAD_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "@app.post(\"/process\")\n",
    "async def process_files(files: List[UploadFile] = File(...)):\n",
    "    results = []\n",
    "    for f in files:\n",
    "        dest = UPLOAD_DIR / f\"{uuid.uuid4()}_{f.filename}\"\n",
    "        with open(dest, \"wb\") as out:\n",
    "            out.write(await f.read())\n",
    "        results.append(process_document(dest))\n",
    "    return {\"results\": results}\n",
    "\"\"\"\n",
    "\n",
    "print(FASTAPI_SNIPPET)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
